{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import librosa\n",
    "import string\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import colorConverter\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "char_list = list(\" '\" + string.ascii_lowercase + '12 ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmax(nd_array):\n",
    "    a = np.max(np.abs(nd_array))\n",
    "    return((-a,a))\n",
    "\n",
    "def elapsed_time(t_start,unit):\n",
    "    t_end = time.time()\n",
    "    d = t_end - t_start\n",
    "    if(unit=='min'):\n",
    "        d /= 60\n",
    "    elif(unit=='h'):\n",
    "        d /= 3600\n",
    "    print('%.2f '%d, unit + ' elapsed',sep='')\n",
    "\n",
    "def build_dict(layer_id, input_tensor, spec, weights, bn_params):\n",
    "    d = dict()\n",
    "    d[input_tensor]=[np.transpose(spec)]\n",
    "    \n",
    "    if layer_id==11:\n",
    "        d['out/bias:0'] = weights[11]\n",
    "        d['out/kernel:0'] = weights[12]\n",
    "        layer_id -= 1\n",
    "    \n",
    "    for i in np.arange(layer_id+1):\n",
    "        d['conv_'+str(i)+'/kernel:0'] = weights[i]\n",
    "        d['batch_norm_'+str(i)+'/beta:0'] = bn_params[(i*4)]\n",
    "        d['batch_norm_'+str(i)+'/gamma:0'] = bn_params[(i*4)+1]\n",
    "        d['batch_norm_'+str(i)+'/moving_mean:0'] = bn_params[(i*4)+2]\n",
    "        d['batch_norm_'+str(i)+'/moving_variance:0'] = bn_params[(i*4)+3]\n",
    "    \n",
    "    return d\n",
    "\n",
    "def conv_layer(inputs, n_filter, kernel_size, stride, layer_id, out_layer=False):\n",
    "    if(not out_layer):\n",
    "        conv = tf.layers.conv1d(\n",
    "            inputs = inputs, \n",
    "            filters = n_filter, \n",
    "            kernel_size = kernel_size,\n",
    "            strides=stride, \n",
    "            activation=None,\n",
    "            use_bias=False, \n",
    "            padding=\"valid\",\n",
    "            name=\"conv_\"+str(layer_id),\n",
    "            data_format=\"channels_last\")\n",
    "\n",
    "        conv_bn = tf.layers.batch_normalization(\n",
    "            conv, \n",
    "            axis=2, \n",
    "            training=False, \n",
    "            name=\"batch_norm_\"+str(layer_id),)\n",
    "\n",
    "        relu_out = tf.nn.relu(conv_bn)\n",
    "\n",
    "        return(conv_bn,relu_out)\n",
    "    else:\n",
    "        conv = tf.layers.conv1d(\n",
    "            inputs = inputs, \n",
    "            filters = n_filter, \n",
    "            kernel_size = kernel_size,\n",
    "            strides=stride, \n",
    "            activation=None,\n",
    "            use_bias=True, \n",
    "            padding=\"valid\",\n",
    "            name=\"out\",\n",
    "            data_format=\"channels_last\")\n",
    "\n",
    "        return(conv)\n",
    "\n",
    "def build_w2l_model(weights,bn_params):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    n_filters = [256,256,256,256,256,256,256,256,256,2048,2048,31]\n",
    "    kernel_sizes = [48,7,7,7,7,7,7,7,7,32,1,1]\n",
    "    strides = [2,1,1,1,1,1,1,1,1,1,1,1]\n",
    "    GRIDS = {16: (4, 4), 32: (8, 4), 64: (8, 8), 128: (16, 8), 256: (16, 16),\n",
    "             512: (32, 16), 1024: (32, 32), 2048: (64, 32), 31: (31,1)}\n",
    "    \n",
    "    x = tf.placeholder(shape=(None,None,128),name='x',dtype=tf.float32)\n",
    "\n",
    "    layer_outs = []\n",
    "    pre_activations = []\n",
    "    p,l = conv_layer(x,n_filters[0],kernel_sizes[0],strides[0],0)\n",
    "    pre_activations.append(p)\n",
    "    layer_outs.append(l)\n",
    "\n",
    "    for i in np.arange(11)[1:]:\n",
    "        p,l = conv_layer(layer_outs[i-1],n_filters[i],kernel_sizes[i],strides[i],i)\n",
    "        pre_activations.append(p)\n",
    "        layer_outs.append(l)\n",
    "\n",
    "    pre_activations.append(conv_layer(layer_outs[10],n_filters[11],kernel_sizes[11],strides[11],11,out_layer=True))\n",
    "    layer_outs.append(tf.nn.softmax(pre_activations[11]))\n",
    "    \n",
    "    # Sensitivity\n",
    "    y = tf.placeholder(shape=(1,31),name='y',dtype=tf.float32)\n",
    "    grads = [tf.gradients(layer_outs[11],x,grad_ys=y)[0][0,:,:]]\n",
    "    \n",
    "    for i in np.arange(11):\n",
    "        grads.append(tf.gradients(layer_outs[11],layer_outs[i],grad_ys=y)[0][0,:,:])\n",
    "    \n",
    "    \n",
    "    return(x,layer_outs, y, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/asr_introspection/w2l_weights.pkl\", \"rb\") as input_file:\n",
    "    weights = pickle.load(input_file)\n",
    "with open(\"/data/asr_introspection/w2l_bn_params.pkl\", \"rb\") as input_file:\n",
    "    bn_params = pickle.load(input_file)\n",
    "\n",
    "spectrogram_in, model_out, relevance_in, sensitivity_out = build_w2l_model(weights, bn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    d = build_dict(11,spectrogram_in,None,weights,bn_params)\n",
    "    \n",
    "    data_dir = \"/data/asr_introspection/\"\n",
    "    input_dir = data_dir + \"spectrogram_input/\"\n",
    "    input_groupmappings = os.listdir(input_dir)\n",
    "    input_groupmappings = [s for s in input_groupmappings if \"group\" in s]\n",
    "    \n",
    "    act_dir = data_dir + \"activations/\"\n",
    "    grad_dir = data_dir + \"gradients/\"\n",
    "    \n",
    "    for f_id, f in enumerate(input_groupmappings):\n",
    "        spectrogram = np.load(input_dir + f[:-17] + \".npy\")\n",
    "        index = int((spectrogram.shape[1]-206)/2) -1\n",
    "        if not os.path.isfile(grad_dir+f[:-17]+\"_grads_pos\"+str(index).zfill(4)+\".pkl\"):\n",
    "            groupmapping = np.load(input_dir + f)\n",
    "            if(groupmapping.shape[0]>0):\n",
    "                d[spectrogram_in] = [np.transpose(spectrogram)]\n",
    "                forward_act = sess.run(model_out,feed_dict=d)\n",
    "                with open(act_dir+f[:-17]+\"_act.pkl\", 'wb') as pf:\n",
    "                    pickle.dump(forward_act, pf, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                # go through all positions\n",
    "                for index in range(int((spectrogram.shape[1]-206)/2)):\n",
    "                    init_relevance = np.zeros_like(forward_act[11][0,0,:])\n",
    "                    init_relevance[np.argmax(forward_act[11][0,index,:])] = 1\n",
    "                    d[spectrogram_in] = [np.transpose(spectrogram[:,(2*index):(2*index)+206])]\n",
    "                    d[relevance_in] = np.reshape(init_relevance,[1,31])\n",
    "                    grads = sess.run(sensitivity_out,feed_dict=d)\n",
    "                    with open(grad_dir+f[:-17]+\"_grads_pos\"+str(index).zfill(4)+\".pkl\", 'wb') as pf:\n",
    "                        pickle.dump(grads, pf, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    if((f_id+1) % 250==0):\n",
    "        print(\"finished \" + str(f_id+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupmapping.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ranges = [206,80,74,68,62,56,50,44,38,32,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.977149 4.637802\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/data/asr_introspection/\"\n",
    "input_dir = data_dir + \"spectrogram_input/\"\n",
    "input_groupmappings = os.listdir(input_dir)\n",
    "input_groupmappings = [s for s in input_groupmappings if \"group\" in s]\n",
    "\n",
    "act_dir = data_dir + \"activations/\"\n",
    "grad_dir = data_dir + \"gradients/\"\n",
    "\n",
    "minimum = np.inf\n",
    "maximum = -np.inf\n",
    "\n",
    "for f_id, f in enumerate(input_groupmappings):\n",
    "    spectrogram = np.load(input_dir + f[:-17] + \".npy\")\n",
    "    if np.min(spectrogram)<minimum: minimum=np.min(spectrogram)\n",
    "    if np.max(spectrogram)>maximum: maximum=np.max(spectrogram)\n",
    "print(minimum, maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.400844 3.539129\n"
     ]
    }
   ],
   "source": [
    "print(np.min(spectrogram),np.max(spectrogram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
