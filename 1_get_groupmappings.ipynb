{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes through the corpus and stores, which snippets are predicted as which letter and phoneme.  \n",
    "(before, all snippets were saved - to save memory, I'll change to writing a mapping to a file)  \n",
    "Information about each snippet is written to a line in a npy file containing:  \n",
    "`start_position,letter,phoneme,phoneme_full/n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import librosa\n",
    "import string\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sys.path.append(\"/project/asr_introspection/jens_w2l/\")\n",
    "from w2l.estimator_main import run_asr\n",
    "from alignment.sequence import Sequence\n",
    "from alignment.vocabulary import Vocabulary\n",
    "from alignment.sequencealigner import SimpleScoring, GlobalSequenceAligner, LocalSequenceAligner\n",
    "from itertools import chain\n",
    "\n",
    "data_config = \"/data/johannsm/introspection_data/mel_new\"\n",
    "model_config = '/data/johannsm/introspection_data/w2l_mel_extralayer'\n",
    "model_dir = \"/data/johannsm/introspection_data/model/\"\n",
    "\n",
    "char_list = list(\" '\" + string.ascii_lowercase + '12 ')\n",
    "\n",
    "def remove_duplicates(s):\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    if len(s) == 1:\n",
    "        return s\n",
    "    if s[0] == s[1]:\n",
    "        return remove_duplicates(s[1:])\n",
    "    return s[0] + remove_duplicates(s[1:])\n",
    "\n",
    "def flatten(listOfLists):\n",
    "    \"Flatten one level of nesting\"\n",
    "    return chain.from_iterable(listOfLists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings_lines = [line.rstrip('\\n') for line in open('/data/johannsm/introspection_data/predictions.phon.txt')]\n",
    "transcription_lines = [line.rstrip('\\n') for line in open('/data/johannsm/introspection_data/valid_predictions.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_tf_random_seed': None, '_save_checkpoints_steps': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff94632c0f0>, '_service': None, '_log_step_count_steps': 100, '_keep_checkpoint_max': 5, '_train_distribute': None, '_task_type': 'worker', '_global_id_in_cluster': 0, '_num_ps_replicas': 0, '_evaluation_master': '', '_num_worker_replicas': 1, '_session_config': None, '_master': '', '_model_dir': '/data/johannsm/introspection_data/model/', '_save_checkpoints_secs': 600, '_task_id': 0, '_keep_checkpoint_every_n_hours': 1, '_save_summary_steps': None}\n",
      "Building input function for ['test-clean', 'test-other', 'test'] set using file /data/johannsm/corpus_new.csv...\n",
      "\t292367 entries found.\n",
      "\tFiltering requested subset...\n",
      "\t5559 entries remaining.\n",
      "\tCreating the dataset...\n",
      "\tBuilding iterator...\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Reading, building and applying model...\n",
      "\tCreating layer layer0 with 1573376 parameters...\n",
      "\tCreating layer layer1 with 459264 parameters...\n",
      "\tCreating layer layer2 with 459264 parameters...\n",
      "\tCreating layer layer3 with 459264 parameters...\n",
      "\tCreating layer layer4 with 459264 parameters...\n",
      "\tCreating layer layer5 with 459264 parameters...\n",
      "\tCreating layer layer6 with 459264 parameters...\n",
      "\tCreating layer layer7 with 459264 parameters...\n",
      "\tCreating layer layer8 with 459264 parameters...\n",
      "\tCreating layer layer9 with 16781312 parameters...\n",
      "\tCreating layer layer10 with 4198400 parameters...\n",
      "Number of model parameters: 26227200\n",
      "\tCreating layer logits with 63488 parameters...\n",
      "WARNING:tensorflow:From /project/asr_introspection/jens_w2l/w2l/estimator_model.py:131: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/johannsm/introspection_data/model/model.ckpt-436503\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:19: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 250 nextlineid 4519\n",
      "finished 500 nextlineid 9430\n",
      "finished 750 nextlineid 15323\n",
      "finished 1000 nextlineid 20355\n",
      "finished 1250 nextlineid 25451\n",
      "finished 1500 nextlineid 32153\n",
      "finished 1750 nextlineid 38352\n",
      "finished 2000 nextlineid 43713\n",
      "finished 2250 nextlineid 49594\n",
      "finished 2500 nextlineid 55684\n",
      "finished 2750 nextlineid 60378\n",
      "finished 3000 nextlineid 65338\n",
      "finished 3250 nextlineid 70136\n",
      "finished 3500 nextlineid 74036\n",
      "finished 3750 nextlineid 78510\n",
      "finished 4000 nextlineid 83283\n",
      "finished 4250 nextlineid 88422\n",
      "finished 4500 nextlineid 94472\n",
      "finished 4750 nextlineid 100129\n",
      "finished 5000 nextlineid 105052\n",
      "finished 5250 nextlineid 110630\n",
      "finished 5500 nextlineid 114788\n"
     ]
    }
   ],
   "source": [
    "preds = run_asr(\"predict\",data_config, model_config, model_dir)\n",
    "\n",
    "sample_start = 0\n",
    "line_id = 0\n",
    "\n",
    "\n",
    "for sample_id in range(5559):\n",
    "    decoded = next(preds)\n",
    "    if(sample_id>=sample_start):\n",
    "    \n",
    "        spectrogram = decoded['all_layers'][0][1][:,:decoded['input_length']]\n",
    "        np.save('/data/asr_introspection/spectrogram_input/sample'+str(sample_id).zfill(4)+'.npy',spectrogram)\n",
    "        \n",
    "\n",
    "        transcription = transcription_lines[sample_id]\n",
    "\n",
    "        for c in '12':\n",
    "            transcription = transcription.replace(c,\" \")\n",
    "        letter_ids = np.where(np.array(list(transcription))!=' ')[0]\n",
    "\n",
    "        transcription = transcription.replace(' ',\"\")\n",
    "        transcription = ''.join([' ' for i in range(50)]) + transcription + ''.join([' ' for i in range(50)])\n",
    "#         print(transcription)\n",
    "\n",
    "        pred = decoded['decoding'][0].replace('  ',' ')\n",
    "        pred = pred.replace('  ',' ')\n",
    "        while(pred[0]==' '):\n",
    "            pred = pred[1:]\n",
    "#         print(pred)\n",
    "\n",
    "        # parse mapping for prediction\n",
    "        pred_split = pred.split(' ')\n",
    "        pred_mapping = []\n",
    "        line_id = line_id+1\n",
    "        for pred_word in pred_split:\n",
    "            word,_,phonemes = mappings_lines[line_id].split(' ')\n",
    "            if(pred_word.lower() != word.lower()):\n",
    "                print(\"Word mismatch\",pred_word.lower(), word.lower())\n",
    "            pred_mapping.append((phonemes+\". \").split('.'))\n",
    "            line_id = line_id + 1\n",
    "        pred_mapping = list(flatten(pred_mapping))[:-1]\n",
    "\n",
    "#         print(str(sample_id).zfill(4),'transcr len:',str(len(transcription)-100).zfill(5))\n",
    "\n",
    "        v = Vocabulary()\n",
    "\n",
    "        aEncoded = v.encodeSequence(Sequence(transcription))\n",
    "        bEncoded = v.encodeSequence(Sequence(pred))\n",
    "\n",
    "        # Create a scoring and align the sequences using global aligner.\n",
    "        scoring = SimpleScoring(2, -1)\n",
    "        aligner = GlobalSequenceAligner(scoring, -1)\n",
    "        score, encodeds = aligner.align(aEncoded, bEncoded, backtrace=True)\n",
    "\n",
    "        # Iterate over optimal alignments and print them.\n",
    "        alignment = v.decodeSequenceAlignment(encodeds[0])\n",
    "#         print(''.join(alignment[:117][0]))\n",
    "#         print(''.join(alignment[:117][1]))\n",
    "#         print('Alignment score:', alignment.score)\n",
    "#         print('Percent identity:', alignment.percentIdentity())\n",
    "\n",
    "        if(len(alignment)>0):\n",
    "            alignment_array = np.array(alignment,dtype='object')\n",
    "            alignment_array = np.concatenate((\n",
    "                    np.reshape(np.repeat(' ',alignment_array.shape[0]),(alignment_array.shape[0],1)),\n",
    "                    alignment_array[:,0:1],    \n",
    "                    np.reshape(np.repeat(' ',alignment_array.shape[0]),(alignment_array.shape[0],1)),                    \n",
    "                    np.reshape(np.repeat(' ',alignment_array.shape[0]),(alignment_array.shape[0],1)),\n",
    "                    alignment_array[:,1:2]),\n",
    "                axis=1)\n",
    "            \n",
    "\n",
    "\n",
    "            pred_id=0\n",
    "            transcr_id = 0\n",
    "            for c_id, c in enumerate(alignment):\n",
    "                if(c[1]!='-'):\n",
    "                    alignment_array[c_id,3] = pred_mapping[pred_id]\n",
    "                    pred_id = pred_id + 1\n",
    "\n",
    "                if(c[0]!='-' and c[0]!=' '):\n",
    "                    alignment_array[c_id,0] = letter_ids[transcr_id]\n",
    "                    transcr_id = transcr_id + 1\n",
    "            \n",
    "            alignment_array[:,2] = [s.replace(\"0\",\"\") for s in alignment_array[:,3]]\n",
    "            alignment_array[:,2] = [s.replace(\"1\",\"\") for s in alignment_array[:,2]]\n",
    "            alignment_array[:,2] = [s.replace(\"2\",\"\") for s in alignment_array[:,2]]\n",
    "\n",
    "            alignment_array = alignment_array[alignment_array[:,0]!=' ',:4]\n",
    "            np.save('/data/asr_introspection/spectrogram_input/sample'+str(sample_id).zfill(4)+'_groupmapping.npy',alignment_array)\n",
    "\n",
    "#             print(\"- success - nextlineid\", str(line_id+1))\n",
    "#         else:\n",
    "#             print(\"- skipped - nextlineid\", str(line_id+1))\n",
    "\n",
    "        line_id = line_id+1\n",
    "    \n",
    "    if((sample_id+1) % 250==0):\n",
    "        print(\"finished \" + str(sample_id+1) + \" nextlineid \" + str(line_id+1))\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
